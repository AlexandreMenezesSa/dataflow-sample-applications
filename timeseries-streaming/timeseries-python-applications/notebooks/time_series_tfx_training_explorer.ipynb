{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC. All Rights Reserved.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the exception of matplotlib all other requirements are the same as \n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries sample notebook\n",
    "\n",
    "This notebook can be used to explore the various stages of data engineering used within the time series library. It has various sections, which correspond to different parts of the ML part of the solution.\n",
    "\n",
    "The Java part of the library is not explored yet, this will be made availabel through Apache Beam xlang transforms at a later date.\n",
    "\n",
    "The notebook will make use of the libraries made avialable via the timeseries python samples. This notebook should be run from within a virtual env that has those samples installed.\n",
    "\n",
    "## [Training](#training)\n",
    "\n",
    "* [Explore ImportGen output](#example)\n",
    "* [Explore StatisticsGen output](#statistics)\n",
    "* [Explore Transform Output](#transform)\n",
    "* [Running Training](#training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import urllib\n",
    "\n",
    "import absl\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "tf.get_logger().propagate = False\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# Import the tfx components we will make use of.\n",
    "import tfx\n",
    "from tfx.components.example_gen.import_example_gen.component import ImportExampleGen\n",
    "from tfx.components import Evaluator\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import Pusher\n",
    "from tfx.components import ResolverNode\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import StatisticsGen\n",
    "from tfx.components import Trainer\n",
    "from tfx.components import Transform\n",
    "\n",
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.proto.evaluator_pb2 import SingleSlicingSpec\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model\n",
    "from tfx.types.standard_artifacts import ModelBlessing\n",
    "\n",
    "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\n",
    "\n",
    "print('TF version', tf.__version__, '\\nTFX version', tfx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id = \"training\"> Training </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = \"example\"> Import Examples </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Data root is the location that the TF.Examples used for training are landed from the Java pipeline. \n",
    "BOOTSTRAP_DATA_ROOT = os.path.expanduser('~/demo/timeseries/data/simple-data-bootstrap/')\n",
    "print(f'The path {BOOTSTRAP_DATA_ROOT} should contain the generated TF.Example files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = InteractiveContext( pipeline_name='SimpleData' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImortExampleGen is used to read the TF.Examples created by the streaming java pipeline.\n",
    "examples = external_input(BOOTSTRAP_DATA_ROOT)\n",
    "example_gen = ImportExampleGen(input=examples) \n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the output from ImportExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can explore the name of the outputs from ImportExampleGen\n",
    "artifact = example_gen.outputs['examples'].get()[0]\n",
    "print(artifact.split_names, artifact.uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URI of the output artifact representing the training examples, which is a directory\n",
    "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "# Iterate over the first 3 records and decode them.\n",
    "# Note that we have both Metadata information and features. \n",
    "# The features will have number of values == the number of timesteps.\n",
    "for tfrecord in dataset.take(3):\n",
    "  serialized_example = tfrecord.numpy()\n",
    "  example = tf.train.Example()\n",
    "  example.ParseFromString(serialized_example)\n",
    "  pp.pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets explore the feature LAST in a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a Dataset with time series examples, extract the LAST value\n",
    "def convert_time_series_data_to_raw_values(dataset: tf.data.Dataset, num_records: int, num_timesteps : int):\n",
    "    for tfrecord in dataset.take(num_records):\n",
    "        serialized_example = tfrecord.numpy()\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(serialized_example)\n",
    "        features = example.features.feature\n",
    "        \n",
    "        output = {}\n",
    "        # Extract the time bounds\n",
    "        output['span_start_timestamp'] = datetime.fromtimestamp(features['METADATA_SPAN_START_TS'].int64_list.value[0] / 1000)\n",
    "        output['span_end_timestamp'] = datetime.fromtimestamp(features['METADATA_SPAN_END_TS'].int64_list.value[0] / 1000)\n",
    "        \n",
    "        for key in features:\n",
    "            if key.endswith('-LAST') or key.endswith('-FIRST'):\n",
    "                output[key] = features[key].float_list.value[num_timesteps-1]\n",
    "        yield output\n",
    "\n",
    "        # Get the URI of the output artifact representing the training examples, which is a directory\n",
    "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "output = convert_time_series_data_to_raw_values(dataset,num_records=28800, num_timesteps=5)\n",
    "df = pd.DataFrame.from_dict(output)\n",
    "df.set_index('span_start_timestamp')\n",
    "df[(df['span_start_timestamp'] > '2000-1-1 01:00:00') & (df['span_start_timestamp'] <= '2000-1-1 01:05:00')].plot('span_end_timestamp',y=['value-LAST','value-FIRST'],figsize=(18,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = \"statistics\"> StatisticsGen </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)\n",
    "# context.show(statistics_gen.outputs['statistics']) # If you want to look at the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=False) # If this is True, the look back is explicitly provided as 4 instead of None\n",
    "context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = \"transform\"> TF Transform </a>\n",
    "\n",
    "Note we are using \n",
    "\n",
    "preprocessing_fn = 'timeseries.encoder_decoder.encoder_decoder_preprocessing.preprocessing_fn'\n",
    "\n",
    "This is the same process function from our pipeline, avoiding duplicating the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    preprocessing_fn = 'timeseries.encoder_decoder.encoder_decoder_preprocessing.preprocessing_fn')\n",
    "context.run(transform, enable_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uri = transform.outputs['transform_graph'].get()[0].uri\n",
    "os.listdir(train_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%skip_for_export\n",
    "\n",
    "# Get the URI of the output artifact representing the transformed examples, which is a directory\n",
    "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "# Iterate over the first 3 records and decode them.\n",
    "for tfrecord in dataset.take(10):\n",
    "  serialized_example = tfrecord.numpy()\n",
    "  example = tf.train.Example()\n",
    "  example.ParseFromString(serialized_example)\n",
    "  pp.pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 5\n",
    "number_features = 2\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "    \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
    "\n",
    "def create_training_data(features):\n",
    "    \"\"\"Extract only one feature for debug\"\"\"\n",
    "    return features['Float32']\n",
    "\n",
    "# Get the URI of the output artifact representing the transformed examples, which is a directory\n",
    "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "transformed_feature_spec = {'Float32': tf.io.FixedLenFeature(shape=[timesteps, number_features], \n",
    "                            dtype=tf.float32, default_value=None), \n",
    "                            'LABEL': tf.io.FixedLenFeature(shape=[timesteps, number_features], \n",
    "                            dtype=tf.float32, default_value=None)}\n",
    "\n",
    "dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern=tfrecord_filenames,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            features=transformed_feature_spec,\n",
    "            reader=_gzip_reader_fn)\n",
    "\n",
    "dataset = dataset.map(create_training_data)\n",
    "\n",
    "for tfrecord in dataset.take(5):\n",
    "    serialized_example = tfrecord.numpy()   \n",
    "    print(serialized_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = \"training\"> Training </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uses user-provided Python function that implements a model using TF-Learn.\n",
    "trainer_args = {\n",
    "        'run_fn': 'timeseries.encoder_decoder.encoder_decoder_run_fn.run_fn',\n",
    "        'transformed_examples': transform.outputs['transformed_examples'],\n",
    "        'schema': schema_gen.outputs['schema'],\n",
    "        'transform_graph': transform.outputs['transform_graph'],\n",
    "        'train_args': trainer_pb2.TrainArgs(num_steps=280),\n",
    "        'eval_args': trainer_pb2.EvalArgs(num_steps=140),\n",
    "        'custom_executor_spec': executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "        'custom_config': {'epochs': 30, \n",
    "                          'train_batches': 1000, \n",
    "                          'eval_batches': 1000, \n",
    "                          'timesteps': 5, \n",
    "                          'number_features': 2, \n",
    "                          'outer_units' : 16, \n",
    "                          'inner_units' : 4},\n",
    "}\n",
    "trainer = Trainer(**trainer_args)\n",
    "context.run(trainer, enable_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
